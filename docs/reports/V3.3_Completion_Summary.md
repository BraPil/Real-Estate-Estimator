# V3.3 Completion Summary

**Version:** 3.3.0  
**Date:** December 9, 2025  
**Branch:** `feature/v3.3-optimization`  
**Tag:** `v3.3.0`

---

## Executive Summary

V3.3 focused on data quality optimization and model tuning. We implemented distressed sales filtering, price capping, temporal features, and Optuna hyperparameter optimization. A critical discovery was data leakage from repeat property sales, leading to honest evaluation using GroupKFold cross-validation.

---

## Performance Comparison

| Metric | V3.2 | V3.3 | Change |
|--------|------|------|--------|
| CV MAE | $144,826 | $115,247 | -20.4% |
| CV R2 | 0.816 | 0.868 | +5.2 pts |
| Test R2 | 0.857 | 0.868* | +1.1 pts |
| Records | 155,855 | 143,476 | -7.9% |
| Features | 43 | 47 | +4 |

*Honest R2 using GroupKFold (no data leakage). Inflated test R2 was 0.966 due to repeat sales.

---

## Key Changes

### 1. Distressed Sales Filter
- Removed bottom 5% of prices per zipcode
- Rationale: Distressed sales (foreclosures, short sales) distort market patterns
- Impact: 7,745 records removed

### 2. Price Cap at $3M
- Removed properties above $3,000,000
- Rationale: Ultra-luxury homes have unique pricing drivers not captured by standard features
- Impact: 4,634 records removed

### 3. Temporal Features
Added 4 new features:
- `sale_year`: Year of sale (2020-2024)
- `sale_month`: Month of sale (1-12)
- `sale_quarter`: Quarter of sale (1-4)
- `sale_dow`: Day of week (0-6)

### 4. Optuna Hyperparameter Tuning
- 30 Bayesian optimization trials
- Best parameters saved to `model/best_params.json`
- Key tuned parameters:
  - `n_estimators`: 1500
  - `max_depth`: 8
  - `learning_rate`: 0.0285
  - `subsample`: 0.858
  - `colsample_bytree`: 0.614
  - `reg_alpha`: 0.0079
  - `reg_lambda`: 6.56

### 5. Data Leakage Discovery
- **Finding:** 12.4% of records are repeat sales of the same property
- **Impact:** Random train/test split leaks information (same property in both)
- **Solution:** GroupKFold CV splits by property ID
- **Result:** Honest R2 = 0.868 vs inflated R2 = 0.966

### 6. CI/CD Updates
- Added `feature/*` branch triggers to CI workflow
- Added `--data-source` flag to `train_with_mlflow.py`
- Train workflow defaults to fresh 2020+ data

---

## Files Created

| File | Purpose |
|------|---------|
| `src/tune_v33.py` | Optuna hyperparameter optimization script |
| `model/best_params.json` | Best hyperparameters from tuning |

## Files Modified

| File | Changes |
|------|---------|
| `src/data/transform_assessment_data.py` | Distressed filter, price cap, temporal features |
| `src/train_fresh_data.py` | Added temporal feature columns |
| `src/evaluate_fresh.py` | Added temporal feature columns |
| `src/train_with_mlflow.py` | Added `--data-source` flag |
| `src/services/feature_service.py` | Temporal feature defaults |
| `tests/test_model.py` | Temporal features in fixtures |
| `.github/workflows/ci.yml` | Feature branch triggers |
| `.github/workflows/train.yml` | Data source input |
| `.gitignore` | Exclude MLflow databases |

---

## Lessons Learned

### 1. Data Leakage is Subtle
Random train/test splits can leak information when the same entity (property) appears multiple times. Always consider entity-level splits for time-series or panel data.

### 2. High R2 is a Warning Sign
When test R2 jumps to 0.966, be skeptical. Investigate before celebrating.

### 3. Honest Evaluation Matters
GroupKFold R2 of 0.868 is still excellent for real estate prediction. The honest metric is more valuable than an inflated one.

### 4. CI/CD Must Evolve with Data
When data sources change (original to fresh), all MLOps scripts must be updated to match.

---

## Model Configuration

```json
{
  "model_type": "XGBRegressor",
  "n_estimators": 1500,
  "max_depth": 8,
  "learning_rate": 0.0285,
  "subsample": 0.858,
  "colsample_bytree": 0.614,
  "reg_alpha": 0.0079,
  "reg_lambda": 6.56,
  "n_features": 47,
  "training_samples": 143476
}
```

---

## Next Steps (V3.4 Candidates)

1. **Log-transform target** - Improve residual distribution for percentage-based errors
2. **Ensemble methods** - Combine XGBoost with LightGBM or CatBoost
3. **Feature selection** - Identify and remove low-importance features
4. **Quantile regression** - Provide prediction intervals

---

**Completed By:** AI Assistant  
**Reviewed By:** Human-in-the-loop
