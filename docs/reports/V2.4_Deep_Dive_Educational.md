# V2.4 Deep Dive: Model Alternatives

**Purpose:** Comprehensive educational guide covering everything accomplished in V2.4  
**Date:** 2025-12-08  
**Audience:** ML Engineers, Data Scientists, and anyone learning ML model selection

---

## Table of Contents
1. [What We Accomplished](#what-we-accomplished)
2. [Why Model Comparison Matters](#why-model-comparison-matters)
3. [The Models We Compared](#the-models-we-compared)
4. [How We Compared Them](#how-we-compared-them)
5. [Understanding the Results](#understanding-the-results)
6. [XGBoost Deep Dive](#xgboost-deep-dive)
7. [Hyperparameter Tuning](#hyperparameter-tuning)
8. [Feature Importance Analysis](#feature-importance-analysis)
9. [Production Deployment](#production-deployment)
10. [Lessons Learned](#lessons-learned)

---

## What We Accomplished

### Before V2.4
- **Model:** K-Nearest Neighbors (KNN) with tuned hyperparameters
- **MAE:** $84,494
- **R²:** 0.7932

### After V2.4
- **Model:** XGBoost with tuned hyperparameters
- **MAE:** $67,041 (**-20.7%**)
- **R²:** 0.8755 (**+10.4%**)

### Total Project Improvement (V1 → V2.4)
- **MAE:** $102,045 → $67,041 (**-34.3%**)
- **R²:** 0.7281 → 0.8755 (**+20.2%**)

---

## Why Model Comparison Matters

### The "No Free Lunch" Theorem
In machine learning, no single algorithm works best for all problems. Different algorithms have different:
- **Inductive biases** - assumptions they make about the data
- **Strengths** - types of patterns they capture well
- **Weaknesses** - situations where they struggle

### When to Compare Models
- After establishing a baseline (we had KNN from V2.3)
- When you have clean, prepared data (our 43 features)
- When you want to squeeze more performance
- Before deploying to production

### What We Learned
KNN was a reasonable baseline, but tree-based models (XGBoost, LightGBM, Random Forest) dramatically outperformed it on this tabular dataset. This is a common pattern in real-world ML.

---

## The Models We Compared

### 1. K-Nearest Neighbors (KNN) - Baseline
**How it works:** For each prediction, find the K most similar houses in the training data and average their prices.

```python
# V2.3 Configuration
KNeighborsRegressor(
    n_neighbors=7,      # Look at 7 nearest neighbors
    weights='distance', # Closer neighbors matter more
    metric='manhattan'  # L1 distance (sum of absolute differences)
)
```

**Pros:**
- Simple and interpretable
- No assumptions about data distribution
- Works well with small datasets

**Cons:**
- Slow at prediction time (must search all training data)
- Sensitive to irrelevant features
- Struggles with high-dimensional data ("curse of dimensionality")

**Our Result:** MAE $84,494

---

### 2. Random Forest
**How it works:** Build many decision trees on random subsets of data and features, then average their predictions.

```python
RandomForestRegressor(
    n_estimators=100,    # 100 trees
    max_depth=20,        # Each tree can be 20 levels deep
    min_samples_split=5, # Need 5+ samples to split a node
    min_samples_leaf=2,  # Leaves must have 2+ samples
    random_state=42,
    n_jobs=-1            # Use all CPU cores
)
```

**Pros:**
- Handles non-linear relationships
- Provides feature importance
- Robust to outliers
- Minimal hyperparameter tuning needed

**Cons:**
- Slower than boosting methods
- Less accurate than XGBoost on most tabular data
- Large model files

**Our Result:** MAE $70,645 (16.4% better than KNN)

---

### 3. XGBoost (Winner!)
**How it works:** Build trees sequentially, where each new tree corrects the errors of the previous trees (gradient boosting).

```python
XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
```

**Pros:**
- Usually best accuracy on tabular data
- Fast training and prediction
- Built-in regularization
- Handles missing values

**Cons:**
- More hyperparameters to tune
- Can overfit if not regularized
- Less interpretable than single trees

**Our Result:** MAE $68,092 → $67,041 after tuning (20.7% better than KNN)

---

### 4. LightGBM
**How it works:** Similar to XGBoost but uses a different tree-building algorithm (leaf-wise instead of level-wise).

```python
LGBMRegressor(
    n_estimators=100,
    max_depth=10,
    learning_rate=0.1,
    num_leaves=31,
    random_state=42
)
```

**Pros:**
- Faster training than XGBoost
- Handles large datasets well
- Lower memory usage

**Cons:**
- Can overfit on small datasets
- More sensitive to hyperparameters

**Our Result:** MAE $68,934 (18.4% better than KNN)

---

### 5. Ridge Regression
**How it works:** Linear regression with L2 regularization to prevent overfitting.

```python
Ridge(alpha=1.0, random_state=42)
```

**Pros:**
- Fast and simple
- Interpretable coefficients
- Good baseline

**Cons:**
- Assumes linear relationships
- Can't capture complex patterns

**Our Result:** MAE $105,267 (worse than KNN - confirms non-linear patterns exist)

---

## How We Compared Them

### Evaluation Framework
```python
# For each model:
# 1. Create pipeline with scaler
pipeline = Pipeline([
    ('scaler', RobustScaler()),
    ('model', model)
])

# 2. Cross-validation on training set (avoid test set peeking)
cv_scores = cross_val_score(
    pipeline, X_train, y_train,
    cv=5,  # 5-fold CV
    scoring='neg_mean_absolute_error'
)

# 3. Fit on full training set
pipeline.fit(X_train, y_train)

# 4. Evaluate on held-out test set
y_pred = pipeline.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
```

### Why This Approach?
1. **RobustScaler** - Handles outliers better than StandardScaler
2. **5-Fold Cross-Validation** - More reliable than single split
3. **Held-out Test Set** - Final unbiased evaluation
4. **Same split for all models** - Fair comparison (random_state=42)

---

## Understanding the Results

### Initial Comparison Table
| Model | CV MAE | Test MAE | Test R² | Time |
|-------|--------|----------|---------|------|
| **XGBoost** | $65,034 | $68,092 | 0.8941 | 3.8s |
| LightGBM | $65,422 | $68,934 | 0.8867 | 3.8s |
| Random Forest | $67,653 | $70,645 | 0.8771 | 35.7s |
| KNN | $79,976 | $84,494 | 0.7932 | 16.5s |
| Ridge | $101,423 | $105,267 | 0.7862 | 0.6s |

### Key Insights

1. **Tree-based models dominate** - XGBoost, LightGBM, and Random Forest all beat KNN by 15-20%

2. **XGBoost vs LightGBM** - Very close ($68k vs $69k). XGBoost slightly better on this dataset.

3. **Random Forest is slowest** - 35.7s vs 3.8s for XGBoost (10x slower!)

4. **Ridge is worst** - Confirms the data has non-linear patterns that linear models can't capture

5. **CV vs Test gap** - All models show CV MAE < Test MAE, which is expected (CV is on training data)

---

## XGBoost Deep Dive

### What is Gradient Boosting?
Gradient boosting builds trees sequentially:

```
Prediction = Tree1 + Tree2 + Tree3 + ... + TreeN

Where each tree corrects the errors of previous trees:
- Tree1: Predicts prices (makes errors)
- Tree2: Predicts Tree1's errors
- Tree3: Predicts remaining errors after Tree2
- ...and so on
```

### Why "Gradient"?
Each tree is fitted to the negative gradient of the loss function (for MAE, this is the sign of the residual).

### XGBoost's Innovations
1. **Regularization** - L1 (reg_alpha) and L2 (reg_lambda) penalties on tree weights
2. **Shrinkage** - Learning rate reduces each tree's contribution
3. **Subsampling** - Random samples and features per tree (reduces overfitting)
4. **Parallelization** - Fast implementation using all CPU cores

### Our Final XGBoost Configuration
```python
XGBRegressor(
    n_estimators=239,      # 239 trees
    max_depth=7,           # Each tree up to 7 levels
    learning_rate=0.0863,  # Small steps (shrinkage)
    subsample=0.7472,      # Use 75% of samples per tree
    colsample_bytree=0.8388, # Use 84% of features per tree
    min_child_weight=6,    # Min samples per leaf
    gamma=0.1589,          # Min loss reduction to split
    reg_alpha=0.2791,      # L1 regularization
    reg_lambda=1.3826      # L2 regularization
)
```

---

## Hyperparameter Tuning

### Why We Used RandomizedSearchCV
- **GridSearchCV** - Tests all combinations (slow for many params)
- **RandomizedSearchCV** - Tests random combinations (faster, often as good)

For XGBoost with 9 hyperparameters, grid search would take hours. Random search with 30 iterations took ~22 seconds.

### Parameter Search Space
```python
{
    'n_estimators': randint(100, 250),
    'max_depth': randint(4, 8),
    'learning_rate': uniform(0.05, 0.15),
    'subsample': uniform(0.7, 0.25),
    'colsample_bytree': uniform(0.7, 0.25),
    'min_child_weight': randint(1, 7),
    'gamma': uniform(0, 0.3),
    'reg_alpha': uniform(0, 0.5),
    'reg_lambda': uniform(0.5, 1.0),
}
```

### Tuning Results
| Metric | Before Tuning | After Tuning | Change |
|--------|---------------|--------------|--------|
| CV MAE | $65,034 | $63,229 | -$1,805 |
| Test MAE | $68,092 | $67,041 | -$1,051 |

Tuning provided ~$1,000-1,800 improvement - worthwhile but not dramatic.

---

## Feature Importance Analysis

### What Feature Importance Tells Us
XGBoost calculates how much each feature contributes to predictions (based on how often it's used in splits and how much it reduces error).

### Top 10 Features (XGBoost)
| Rank | Feature | Importance | Interpretation |
|------|---------|------------|----------------|
| 1 | `grade` | 0.2010 | Construction quality is #1 price driver |
| 2 | `hous_val_amt` | 0.1480 | Zipcode median house value |
| 3 | `per_bchlr` | 0.1176 | % with bachelor's degree (neighborhood quality) |
| 4 | `waterfront` | 0.1070 | Waterfront premium is huge |
| 5 | `sqft_living` | 0.1025 | Size matters |
| 6 | `per_prfsnl` | 0.0925 | % professional workers |
| 7 | `medn_incm_per_prsn_amt` | 0.0431 | Income level |
| 8 | `view` | 0.0273 | View quality |
| 9 | `lat` | 0.0140 | Latitude (location) |
| 10 | `bathrooms` | 0.0123 | Bathroom count |

### Key Insight
**Demographics features (3 of top 6)** validate the V2.1 decision to add zipcode demographics. Features like `per_bchlr` (% with bachelor's degree) are strong proxies for neighborhood desirability.

---

## Production Deployment

### Model Pipeline Architecture
```
Input Features (43)
       ↓
┌─────────────────┐
│  RobustScaler   │  ← Normalize features
└─────────────────┘
       ↓
┌─────────────────┐
│   XGBRegressor  │  ← Make prediction
└─────────────────┘
       ↓
  Price Prediction
```

### File Structure
```
model/
├── model.pkl                      # Production model (XGBoost pipeline)
├── model_v2.4.1_xgboost_tuned.pkl # Backup of tuned model
├── model_features.json            # 43 feature names in order
├── metrics.json                   # Current performance metrics
└── evaluation_report.json         # Detailed evaluation report
```

### API Compatibility
The sklearn Pipeline interface is model-agnostic:
```python
# Works the same for KNN or XGBoost
pipeline.predict(features)
```
No API changes needed - just swap the model file!

---

## Lessons Learned

### 1. Tree-Based Models Win on Tabular Data
This is a well-known pattern. For structured/tabular data, gradient boosting (XGBoost, LightGBM) almost always beats:
- KNN
- Linear models
- Neural networks (usually)

### 2. Start Simple, Then Compare
Our progression was ideal:
- V1: Simple KNN baseline
- V2.3: Tuned KNN (squeezed out improvements)
- V2.4: Compare alternatives → Found 20% improvement

### 3. Hyperparameter Tuning Has Diminishing Returns
- Default XGBoost: $68,092 MAE
- Tuned XGBoost: $67,041 MAE
- Improvement: ~$1,000 (1.5%)

The model choice mattered more than hyperparameters.

### 4. Feature Importance Validates Feature Engineering
The demographics features added in V2.1 turned out to be among the most important predictors. Data-driven feature selection confirmed our domain-knowledge decisions.

### 5. Always Verify Deployment
We discovered the model.pkl wasn't updated correctly - the old KNN was still being used. Always verify:
```python
# Check what model is actually deployed
import pickle
m = pickle.load(open('model/model.pkl', 'rb'))
print(type(m.named_steps['model']))
```

### 6. Constrain Parameter Spaces
LightGBM with wide parameter ranges (num_leaves: 15-100) caused 30+ minute runs. Focused ranges run in seconds.

---

## Scripts Created

| Script | Purpose |
|--------|---------|
| `src/compare_models.py` | Compare 5 models with CV and test evaluation |
| `src/tune_xgboost.py` | RandomizedSearchCV for XGBoost tuning |
| `src/tune_top_models.py` | Multi-model tuning (slower, less practical) |

### Running Model Comparison
```powershell
# Full comparison (5 models)
python src/compare_models.py

# XGBoost tuning
python src/tune_xgboost.py --n-iter 30
```

---

## Summary

V2.4 demonstrated a fundamental ML workflow:
1. **Establish baseline** (KNN from V2.3)
2. **Compare alternatives** (5 models)
3. **Select best performer** (XGBoost)
4. **Tune hyperparameters** (RandomizedSearchCV)
5. **Validate and deploy** (evaluate.py, API test)
6. **Document everything** (you're reading it!)

**The result:** A 20.7% improvement in MAE, taking total project improvement to 34.3%.

---

**Document Version:** 1.0  
**Last Updated:** 2025-12-08  
**Author:** AI Assistant + Human Collaboration
