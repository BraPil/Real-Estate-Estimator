# V2.4: Model Alternatives Comparison

**Status:** Ready to Run  
**Date Created:** 2025-12-08  
**Script:** `src/compare_models.py`

---

## Overview

V2.4 compares the tuned KNN model (V2.3 baseline) against alternative model types to find the best performer for real estate price prediction.

### Models Evaluated

| Model | Description | Expected Behavior |
|-------|-------------|-------------------|
| **KNN (V2.3)** | n_neighbors=7, weights=distance, metric=manhattan | Baseline - current production |
| **Random Forest** | 100 trees, max_depth=20 | Good for non-linear relationships |
| **XGBoost** | 100 estimators, learning_rate=0.1 | Often best on tabular data |
| **LightGBM** | 100 estimators, num_leaves=31 | Fast, handles large data |
| **Ridge** | alpha=1.0 | Linear baseline |

---

## Prerequisites

Install XGBoost and LightGBM if not already installed:

```powershell
pip install xgboost lightgbm
```

Or update all dependencies:

```powershell
pip install -r requirements.txt
```

---

## Running the Comparison

### Basic Usage

```powershell
cd c:\Experiments\Real-Estate-Estimator
python src/compare_models.py
```

### With MLflow Tracking

```powershell
python src/compare_models.py --experiment-name real-estate-v2.4-comparison
```

---

## Expected Output

The script will:

1. **Load Data** - Same 21,613 samples, 43 features as V2.3
2. **Evaluate Each Model** with 5-fold cross-validation
3. **Print Comparison Table** showing:
   - CV MAE (mean ± std)
   - Test MAE, R², RMSE
   - Training time
4. **Feature Importance** from tree-based models
5. **Recommendation** based on results

### Output Files

| File | Location | Description |
|------|----------|-------------|
| `v2.4_model_comparison_*.csv` | `logs/` | Full results table |
| `v2.4_feature_importance_*.csv` | `logs/` | Feature rankings |
| `comparison_results.json` | `model/` | Summary JSON |

---

## Expected Results

Based on typical tabular data performance:

| Model | Expected Test MAE | Notes |
|-------|-------------------|-------|
| KNN (baseline) | ~$84,494 | V2.3 tuned |
| Random Forest | ~$75,000-82,000 | Usually 5-10% better |
| XGBoost | ~$72,000-80,000 | Often best |
| LightGBM | ~$73,000-81,000 | Similar to XGBoost |
| Ridge | ~$100,000+ | Linear model, worse |

**Note:** Tree-based models typically outperform KNN by 5-15% on tabular data.

---

## Next Steps After Running

### If Tree Model Wins (>5% improvement)

1. Run hyperparameter tuning on best model
2. Update `model/model.pkl` with tuned version
3. Update `model/metrics.json`
4. API should work without changes (Pipeline interface)

### If KNN Remains Best (<5% difference)

1. Keep current V2.3 model
2. Document findings
3. Consider V2.5 (Robust Evaluation) next

---

## Feature Importance Insights

Tree models provide feature importance rankings, revealing:
- Which features drive predictions most
- Potential features to engineer or remove
- Business insights about price drivers

**Expected Top Features:**
1. `sqft_living` - Living area size
2. `grade` - Construction quality
3. `lat` / `long` - Location
4. `sqft_living15` - Neighborhood context
5. `bathrooms` - Bathroom count

---

## Troubleshooting

### XGBoost/LightGBM Not Found
```powershell
pip install xgboost lightgbm
```

### Memory Error
Reduce data or use subset:
```python
# In compare_models.py, add after loading data:
X = X.sample(n=10000, random_state=42)
```

### MLflow Issues
MLflow is optional; script works without it.

---

**Document Version:** 1.0  
**Last Updated:** 2025-12-08
