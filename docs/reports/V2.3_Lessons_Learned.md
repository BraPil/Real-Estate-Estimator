# V2.3 Lessons Learned

**Project:** Real Estate Price Predictor  
**Date:** 2025-12-08  
**Phase:** V2.3 - Hyperparameter Tuning

---

## Summary

V2.3 used GridSearchCV to find optimal KNN hyperparameters, improving MAE by 5.9%. This document captures key lessons learned during implementation.

---

## Lesson 1: Default Hyperparameters Are Rarely Optimal

### The Problem
scikit-learn uses sensible defaults (`n_neighbors=5`, `weights='uniform'`), but these are general-purpose, not tuned for our specific problem.

### What We Found
| Parameter | Default | Optimal | Impact |
|-----------|---------|---------|--------|
| n_neighbors | 5 | 7 | Minor |
| weights | uniform | **distance** | **Major** |
| metric | minkowski | **manhattan** | **Major** |

### Takeaway
> Always tune hyperparameters. Default values can leave 5-10% performance on the table.

---

## Lesson 2: Distance Weighting Matters for KNN

### The Problem
With `weights='uniform'`, all k neighbors count equally, regardless of how close they are.

### Why Distance Weighting Helps
```
Scenario: Predicting price for a 2000 sqft home
Neighbor 1: 2001 sqft, $400k (distance: 0.1)
Neighbor 2: 3000 sqft, $600k (distance: 100)

Uniform:  ($400k + $600k) / 2 = $500k ← Wrong!
Distance: ($400k × 10 + $600k × 0.01) / 10.01 ≈ $400k ← Better!
```

### Takeaway
> Use `weights='distance'` for KNN regression. Closer neighbors are more similar and should have more influence.

---

## Lesson 3: Manhattan Distance Beats Euclidean in High Dimensions

### The Problem
With 43 features, Euclidean distance suffers from the "curse of dimensionality."

### Why Manhattan is Better
In high dimensions:
- Euclidean: All points appear almost equidistant
- Manhattan: Distances remain discriminative

**Mathematical intuition:**
```
Euclidean: √(Σ(xᵢ - yᵢ)²)  ← Squaring amplifies noise
Manhattan: Σ|xᵢ - yᵢ|      ← Linear, more robust
```

### Evidence
Our grid search confirmed: `manhattan` beat `euclidean` across all k values.

### Takeaway
> For tabular data with many features, try Manhattan distance first. It's often better than Euclidean.

---

## Lesson 4: Cross-Validation Prevents Overfitting to Test Set

### The Problem
In V2.1.2, we discovered patterns by looking at test set performance. This contaminated our evaluation.

### What We Did Differently in V2.3
```
V2.1.2 (Bad):
1. Look at test set results
2. Find pattern
3. Evaluate on same test set
→ Contaminated estimate

V2.3 (Good):
1. Use only training data for tuning (via CV)
2. Select best model
3. Evaluate ONCE on test set
→ Unbiased estimate
```

### Takeaway
> Use cross-validation for model selection. Touch the test set only for final evaluation.

---

## Lesson 5: GridSearchCV is Parallelizable and Fast

### What We Expected
126 combinations × 5 folds = 630 fits  
Estimated: 1-2 hours

### What We Got
```python
GridSearchCV(..., n_jobs=-1)  # Use all CPU cores
```
**Actual runtime: 24.5 minutes**

### Takeaway
> Don't fear exhaustive grid search for small parameter spaces. Parallel execution makes it practical.

---

## Lesson 6: Overfitting Gap is a Key Health Metric

### What We Measured
```
CV MAE (training): $79,976
Test MAE (unseen): $84,494
Gap: -$4,517 (-5.6%)
```

### Interpretation
- Gap < 5%: ✅ Great generalization
- Gap 5-10%: ⚠️ Acceptable
- Gap > 10%: ❌ Overfitting problem

### Why This Matters
If CV score >> Test score, the model memorized training data and won't generalize.

### Takeaway
> Always report CV vs Test gap. It's more informative than raw accuracy.

---

## Lesson 7: Hyperparameter Tuning Has Diminishing Returns

### Our Journey
| Version | Test MAE | Improvement |
|---------|----------|-------------|
| Original → V1 | -$18k | -15% |
| V1 → V2.1 | -$12k | -12% |
| V2.1 → V2.3 | -$5k | -6% |

**Pattern:** Each improvement is smaller than the last.

### Why This Happens
- Easy wins (adding features) come first
- Hyperparameter tuning squeezes out remaining value
- Eventually, model architecture limits further gains

### Takeaway
> After hyperparameter tuning, the next big gain usually requires changing the model type (V2.4: Random Forest, XGBoost).

---

## Lesson 8: Document Parameter Search Space

### What We Logged
```python
param_grid = {
    'n_neighbors': [3, 5, 7, 10, 15, 20, 30],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2, 3],
}
```

### Why This Matters
- Reproducibility: Others can replicate
- Debugging: Know what was tested
- Future work: Know what to expand

### Takeaway
> Always log the full parameter grid, not just the best result.

---

## Key Metrics Summary

| What We Learned | Metric Impact |
|-----------------|---------------|
| Distance weighting | MAE: -$3-4k (estimated) |
| Manhattan distance | MAE: -$1-2k (estimated) |
| n_neighbors=7 | MAE: -$0.5k (estimated) |
| **Combined** | **MAE: -$5,275 (-5.9%)** |

---

## What We'd Do Differently

1. **Test more k values** - Could try 8, 9 for fine-tuning
2. **Add algorithm parameter** - `algorithm='auto'` vs `ball_tree` vs `kd_tree`
3. **Try leaf_size tuning** - Affects speed, not accuracy
4. **Log training time** - Useful for production planning

---

## Recommendations for V2.4+

1. **V2.4 (Model Alternatives):** Try Random Forest, XGBoost - likely 10-20% better
2. **V2.5 (Robust Evaluation):** Add bootstrap confidence intervals
3. **Consider:** Bayesian hyperparameter optimization for complex models

---

**Document Version:** 1.0  
**Last Updated:** 2025-12-08
